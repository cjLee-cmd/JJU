import torch
import chromadb
from transformers import AutoTokenizer, AutoModel
from config import HUGGINGFACE_API_KEY  

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (PersistentClient ì‚¬ìš©)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# "sentence-transformers/all-MiniLM-L6-v2" ëª¨ë¸ì„ ì‚¬ìš© (384ì°¨ì› ì¶œë ¥ ëª¨ë¸)
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2", use_auth_token=HUGGINGFACE_API_KEY)
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2", use_auth_token=HUGGINGFACE_API_KEY)

def mean_pooling(model_output, attention_mask):
    """
    ëª¨ë¸ ì¶œë ¥(token embeddings)ì— ëŒ€í•´ mean poolingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    attention_maskë¥¼ ê³ ë ¤í•˜ì—¬ ê° í† í° ì„ë² ë”©ì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    token_embeddings = model_output[0]  # í† í° ì„ë² ë”©
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    return sum_embeddings / sum_mask

def generate_embeddings(document_chunks):
    """
    ë¬¸ì„œ ì¡°ê° ë¦¬ìŠ¤íŠ¸(document_chunks)ì— ëŒ€í•´ Hugging Faceì˜ transformers ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ê° ë¬¸ì„œ ì¡°ê°ì— ëŒ€í•´:
      - í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ê³ ,
      - ëª¨ë¸ì„ í†µí•´ í† í° ì„ë² ë”©ì„ ìƒì„±í•œ í›„,
      - mean poolingì„ í†µí•´ í•˜ë‚˜ì˜ ì„ë² ë”© ë²¡í„°ë¡œ ë§Œë“­ë‹ˆë‹¤.
      
    ë°˜í™˜: ê° ë¬¸ì„œ ì¡°ê°ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°(NumPy ë°°ì—´) ë¦¬ìŠ¤íŠ¸.
    """
    embeddings = []
    for text in document_chunks:
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì¦ˆ (padding ë° truncation ì ìš©)
        encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
        # ëª¨ë¸ ì¶”ë¡  (gradient ê³„ì‚° ë¶ˆí•„ìš”)
        with torch.no_grad():
            model_output = model(**encoded_input)
        # mean poolingìœ¼ë¡œ ë‹¨ì¼ ì„ë² ë”© ë²¡í„° ê³„ì‚°
        embedding = mean_pooling(model_output, encoded_input['attention_mask'])
        embeddings.append(embedding.squeeze(0).cpu().numpy())
    return embeddings

def reset_chroma_collection():
    """
    ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜("document_embeddings")ì„ ì‚­ì œí•˜ê³  ìƒˆ ì»¬ë ‰ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        chroma_client.delete_collection(name="document_embeddings")
        print("ğŸ”„ ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    collection = chroma_client.create_collection(name="document_embeddings", metadata={"dimension": 384})
    print("âœ… ìƒˆë¡œìš´ ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ (dim=384)!")
    return collection

def save_embeddings_to_chromadb(embeddings, document_chunks):
    """
    ìƒì„±ëœ ì„ë² ë”© ë²¡í„°ì™€ í•´ë‹¹ ë¬¸ì„œ ì¡°ê°ì„ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    ê° ë¬¸ì„œ ì¡°ê°ì— ëŒ€í•´ ê³ ìœ  ID ë° ë©”íƒ€ë°ì´í„°(í…ìŠ¤íŠ¸)ë¥¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
    """
    collection = reset_chroma_collection()
    for i, (embedding, text) in enumerate(zip(embeddings, document_chunks)):
        if len(embedding) != 384:
            print(f"âš ï¸ ê²½ê³ : ì„ë² ë”© ë²¡í„° ì°¨ì›ì´ {len(embedding)}ì…ë‹ˆë‹¤. 384ë¡œ ë³€í™˜ í•„ìš”!")
            continue
        collection.add(
            ids=[f"doc_{i}"],
            embeddings=[embedding.tolist()],
            metadatas=[{"text": text}]
        )
    print("âœ… ChromaDBì— ì„ë² ë”© ì €ì¥ ì™„ë£Œ!")